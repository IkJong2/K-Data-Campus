{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20190731_박종익_RNN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"QP202V0hcZVp","colab_type":"text"},"source":["# RNN \n","- Recurrent Neural Networks\n","\n","## FFN(Feed Forward Neural Networks)\n","- FFN은 각 피쳐들의 조합을 통해 추론\n","- but FFN은 시간적/순서적 정보를 고려할 수 없음!\n","\n","## RNN - Sequence\n","1. 각 인풋이 들어올 때마다 그 때의 상태(벡터)를 계산\n","2. 예측할 순간의 상태를 이용하여 예측\n","\n","PDF 13p 보면 순회가능한 아이라서 Recurrent\n","\n","이전 히든 스페이스 벡터와 현재 인풋 값을 하기 때문에 \n","\n","$$ h_t = tanh(Ux_t + Ws_{t-1}) ... o_t = Vs_t $$\n","\n","x_t는 현재 인풋 값이고, s_t-1이 이전 상태(state) 벡터이다. o_t는 아웃풋 값\n","\n","- one-to-one : FFN과 같음\n","\n","- one-to-many : Image Captioning, Generation 등\n","\n","- many-to-many : Machine translation, QA 등\n","\n","- many-to-many : Video recognition\n","\n","RNNs - Backpropagation through Time (**BPTT**)\n","\n","$$ W = \\partial L2 + \\partial L1 +\\partial L0$$\n","\n","결국 W는 모든 요소들에 의해서 영향을 받게 된다.\n","\n","하지만, 학습이 가면 갈 수록, 초기 상태 벡터는 거듭된 매트릭스 곱으로 0에 수렴하게 된다!!\n","\n","결국 최근 값에 대한 정보가 weight가 크게 받게 된다!!\n","\n","pdf 18p\n","\n","|W|<1 이면 계속 곱해지기 때문에 0으로 감 Gradient Vanishing\n","\n","|W|>1 이면 계속 곱해지면 Inf으로 감 Gradient Exploding\n","\n","- RNN의 가장 큰 장점!!\n","  - Sequence를 고려하여 할 수 있다!! 시간적 연산이 가능~\n","  - Sequential한 정보를 잘 표현을 한다\n","  - 문장 표현을 가장 쉽게 할 수 있다.\n","  - output 같은 경우 길이를 무한정으로 계속 생산 해낼 수 있다.\n","  \n","- RNN의 가장 큰 단점?\n","  - 느.리.다 (FFN에 비해 n번 돈다)\n","    - e.g. 500개의 단어?? 500번을 왔다갔다 해야함\n","  - Time이 늘어날 수록 이전 값들이 사라지는 경향\n","    - 해결책? 맨 처음 망을 지날 때 두가지 방향을 만듬!\n","    - 실제로 이렇게 하였을 때 성능이 더 좋았다.\n","      1. 정방향 하나\n","      2. 역방향 하나\n","  - **Gradient Vanishing/Exploding의 위험이 따른다**\n","    - but 규제를 줌으로써 해결책이 존재하긴 함!\n","    \n","\n","- LSTM (Long-Short Term Memory)\n","  1. Forget Gate\n","    - 이전 상태를 얼마나 잊을지\n","    - 일반적으로 Sigmoid를 통해서 살릴지 말지를 정함\n","  2. Input Gate\n","    - 현재 입력 정보를 얼마나 받을지\n","  3. Output Gate\n","    - 새로 계산된 정보를 얼마나 상태정보로 출력할지\n","\n","이러한 작업들로 인해 Gradient Vanishing/Exploding을 방지해줌!! \n","by +연산을 통해서...  이전 값에 W를 주지 않고 그대로 하나를 가지고옴!\n","$$ f(x_t) = Wx_{t-1} + x_{t-1} $$\n","...이걸 통해서 Gradient Vanishing은 해결이 가능\n","\n","Gradient Exploding의 경우, Upper boundary를 줌으로써 이 이상을 넘어가지 못하게 한다.\n","\n","1. e.g. 0.8의 경우 0.5로 한정을 줘버린다. \n","2. e.g. Norm으로 나눠 줌으로써 0-1 값으로 주어버림\n","\n","- GRU (Gated Recurrent Unit)\n","  1. Update Gate\n","  2. Forget Gate"]},{"cell_type":"markdown","metadata":{"id":"yyqYx3QZsEYD","colab_type":"text"},"source":["RNN에서 각 Layer의 자료구조가\n","\n","나/는/ 학교/에\n"," \n"," 워드 임베딩을 통해서\n"," \n"," T = 4가 된다\n"," \n"," B,d는 벡터화를 통해서 '나'라는 단어로 이루어진 벡터들!\n"," \n"," Batch의 경우 문장마다 길이가 다르다.\n"," \n"," 어떤거는 길이가 10 어떤건 길이가 4\n"," \n"," 배치 내에서 길이가 가장 큰 아이로 모두 맞춰 준다. how?? padding을 통해서 모두 0으로 맞춰주면 Tensor가 6면체 형태로 생성이 되게 된다.\n"," \n"," 배치내에서 만들지 말고, 그 데이터 셋 내에서 가장 긴 아이로 맞출 것.\n"," \n"," 이런 것들은 찾으면서 하면서 만들어야 할 것이다...\n"," "]},{"cell_type":"markdown","metadata":{"id":"BzZ56hjvqSMf","colab_type":"text"},"source":["RNN은\n","\n","Layer는 해보면서 적당히 쌓을 것...\n","\n","안그래도 느린데 더 느려진다~~\n","\n","bidirectional 옵션은 정방향 역방향으로 갈지!\n","\n","정방향 h3, 역방향 h3 가 나오면 concat을 통해서 Dimension이 두배로 늘어난다.\n","\n","조심할 것! batch_first\n","\n"," T x B x d 순서를 True로 주게 되면\n"," \n"," B x T x d 순서로 넣어주면 된다~\n"," \n"," LSTM의 경우\n"," \n"," "]},{"cell_type":"markdown","metadata":{"id":"x7VDnD1Mt6_c","colab_type":"text"},"source":["CNN의 경우 Max Pooling을 사용하여서 Feature 추출이 가능!\n","\n","...이 경우 2: 20, 3:20, 4:20 -> 60 dimension으로 Classification\n","\n","이 영화 정말 재밌다 T=4\n","\n","임베딩 벡터는 d dimmension으로 이루어져 있을 것\n","\n","2짜리 filter, 3짜리 filter\n","\n","2 filter로 slicing window하면 길이가 3짜리 벡터가 나올 것!\n","\n","3짜리 벡터에서 Max Pooling 과정을 택하면 1짜리 벡터가 나옴!\n","\n","3 filter로 slicing window하면 길이가 2짜리 벡터가 나올 것!\n","\n","2짜리 벡터에서 Max Pooling 과정을 택하면 1짜리 벡터가 나옴!\n","\n","이 두개를 concat을 통해서 길이가 2짜리 벡터가 나옴!!\n","\n","결국 4개를 2개의 필터를 가지고 길이가 2짜리 벡터를 생성해냄!\n","\n","이런식으로 Feature를 잡은 다음에 Linear Regression을 하던지 다른 머신러닝을 통해 점수를 산출해낸다!\n","\n","- 장점\n","  - 빠르다\n","  - Gradient Vanishing/Exploding\n","  \n","- 단점\n","  - 문장이 마지막에 확 반전이 일어날 경우 잡아내기 힘들다.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"TwoWwIGCz37J","colab_type":"code","outputId":"40387891-7503-4651-c978-ea39ecef94a7","executionInfo":{"status":"ok","timestamp":1564563441331,"user_tz":-540,"elapsed":633,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/drive/')"],"execution_count":108,"outputs":[{"output_type":"stream","text":["Drive already mounted at /drive/; to attempt to forcibly remount, call drive.mount(\"/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PPkUXVJc0Iq6","colab_type":"code","colab":{}},"source":["!apt-get update\n","!apt-get install g++ openjdk-8-jdk"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FqwOtvIi4Ujp","colab_type":"code","outputId":"06296a56-5e99-4c4e-9f0b-c7e07383a788","executionInfo":{"status":"ok","timestamp":1564557277161,"user_tz":-540,"elapsed":5971,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}},"colab":{"base_uri":"https://localhost:8080/","height":178}},"source":["!pip install konlpy"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting konlpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/3d/4e983cd98d87b50b2ab0387d73fa946f745aa8164e8888a714d5129f9765/konlpy-0.5.1-py2.py3-none-any.whl (19.4MB)\n","\u001b[K     |████████████████████████████████| 19.4MB 1.7MB/s \n","\u001b[?25hCollecting JPype1>=0.5.7 (from konlpy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n","\u001b[K     |████████████████████████████████| 2.7MB 55.6MB/s \n","\u001b[?25hInstalling collected packages: JPype1, konlpy\n","Successfully installed JPype1-0.7.0 konlpy-0.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C_rzpIPG4YfX","colab_type":"code","outputId":"1abb698f-0303-45a7-d895-31f031e20cb7","executionInfo":{"status":"ok","timestamp":1564557511693,"user_tz":-540,"elapsed":2357,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!ls \"/drive/My Drive/Colab Notebooks/data\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["data_deep.pkl  data_ml.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"40NbOpqr0i9W","colab_type":"code","colab":{}},"source":["from konlpy.tag import Okt\n","import torch ##파이토치 사용~\n","import torch.nn as nn # 이 아이를 많이 쓸 것! 여기에 다양한 layer들이 들어있음\n","import torch.nn.functional as F # 일반적인 Activation function이라던지 많이 사용하는 function들이 들어있음\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","\n","import numpy as np\n","import pickle\n","import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4oFtXW9n4ROh","colab_type":"code","colab":{}},"source":["with open('/drive/My Drive/Colab Notebooks/data/data_deep.pkl','rb') as f:\n","  index2voca = pickle.load(f)\n","  voca2index = pickle.load(f)\n","  train_X = pickle.load(f)\n","  train_y = pickle.load(f)\n","  test_X = pickle.load(f)\n","  test_y = pickle.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YfoThOBq6o_z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5c6bf06f-dda5-4ee6-8a2a-b8d7182aa38e","executionInfo":{"status":"ok","timestamp":1564558333953,"user_tz":-540,"elapsed":577,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}}},"source":["index2voca[50]"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'안'"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"fhj41kb47bkR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4d386e7f-0ba5-430b-9b73-6e640a34f432","executionInfo":{"status":"ok","timestamp":1564558334794,"user_tz":-540,"elapsed":419,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}}},"source":["voca2index[\"영화\"]"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"1fjk79N87me4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b27c9f87-fb4d-4a61-ff0e-20e0c9636563","executionInfo":{"status":"ok","timestamp":1564558335645,"user_tz":-540,"elapsed":384,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}}},"source":["train_X.shape, train_y.shape, test_X.shape, test_y.shape"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((251880, 100), (251880,), (62970, 100), (62970,))"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"BUzdI61C78es","colab_type":"code","colab":{}},"source":["train_X, test_X = torch.from_numpy(train_X), torch.from_numpy(test_X)\n","train_y, test_y = torch.from_numpy(train_y), torch.from_numpy(test_y)\n","\n","train_X, test_X = train_X.long(), test_X.long()\n","train_y, test_y = train_y.view(-1,1), test_y.view(-1,1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tbDC-tZU6sIJ","colab_type":"code","colab":{}},"source":["train_dataset = TensorDataset(train_X, train_y)\n","test_dataset = TensorDataset(test_X, test_y)\n","\n","##Dataset 만들고 Loader를 만들어주어야 한다~~\n","\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","## train은 셔플을 해주어야 한다!\n","test_loader = DataLoader(test_dataset, batch_size=128)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w1HS3XJx9AUT","colab_type":"code","colab":{}},"source":["class CNNRegressor(nn.Module):\n","  def __init__(self, voca_num, embedding_dim, filter_lengths=[2, 3, 4],\n","              filter_num=20):\n","    super(CNNRegressor, self).__init__()\n","    \n","    self.voca_num = voca_num\n","    self.embedding_dim = embedding_dim\n","    self.filter_lengths = filter_lengths\n","    self.filter_num = filter_num\n","    \n","    ## embedding 정의\n","    self.embedding = nn.Embedding(self.voca_num, self.embedding_dim,\n","                                  padding_idx=0)\n","    \n","    ##CNN filter 정의 image에서는 3 but 여기는 1!\n","    \n","    self.filters = nn.ModuleList([nn.Conv1d(1, self.filter_num, (i, embedding_dim))  ##embedding이라서 1!\n","                                  for i in filter_lengths])\n","    ##파이투치에서는 리스트로 사용할 수가 없다.\n","    \n","    ##max pooling 후에 60 dimmension이 생긴다!\n","    self.linear1 = nn.Linear(len(self.filter_lengths) * self.filter_num, 10) ##필터의 종류 * 필터의 개수\n","    ##한 Layer를 더 쌓는 것\n","    self.linear2 = nn.Linear(10,1) ##이건 그냥 아웃풋 용!\n","  \n","  def forward(self, word_indices):\n","    embs = self.embedding(word_indices) ##word_indices는 size가 Batch * T\n","    ##embs 는 Batch * T * d 우리는 B * 1 * T * d 만들어줘야함!\n","    embs = embs.unsqueeze(1)\n","    \n","    out = [torch.relu(conv(embs)) for conv in self.filters]\n","    # print(out[0].shape) ##여기서는 3 dim을 요구하는데, 4dim이라서 에러가 나온다... 마지막 1! 그래서 squeeze\n","    out = [o.squeeze(3) for o in out]\n","    out = [F.max_pool1d(feature, feature.size(2)) for feature in out] ## 99에 해당하는 index 2를 써야함\n","    ## 여기서 concat해서 filter들 붙여주기\n","    out = torch.cat(out, dim=1) ##개수가 총 필터의 개수만큼 된다.\n","    out = out.squeeze(2)\n","    out = torch.relu(self.linear1(out))\n","    out = self.linear2(out)\n","    \n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cljksFOtB4wQ","colab_type":"code","colab":{}},"source":["device = torch.device = 'cuda'\n","regressor = CNNRegressor(len(index2voca), 128)\n","regressor = regressor.to(device)\n","\n","mse_loss = nn.MSELoss()\n","optimizer = torch.optim.Adam(regressor.parameters(), lr=0.01)\n","#voca_num, embedding_dim, filter_lengths=[2, 3, 4],filter_num=20"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"teA_9gy8IerY","colab_type":"code","colab":{}},"source":["#for batch_X, batch_y in train_loader:\n","#  predict = regressor(batch_X)\n","#  break\n","##예상된 사이즈랑 다르다!\n","##디버깅을 위한 툴!!"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zni1aO4oIau0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":897},"outputId":"bec9d395-a1b7-481e-dd91-0b8f1cade545","executionInfo":{"status":"ok","timestamp":1564561757577,"user_tz":-540,"elapsed":146475,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}}},"source":["epochs = 5\n","\n","regressor.train()\n","for e in range(epochs):\n","    for i, (batch_X, batch_y) in enumerate(train_loader):\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        \n","        predict = regressor(batch_X)\n","        \n","        loss = mse_loss(predict, batch_y)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if i % 200 == 0:\n","            loss = loss.item()\n","            print(f\"{e}epochs, {i}iters - {loss}\")"],"execution_count":59,"outputs":[{"output_type":"stream","text":["0epochs, 0iters - 0.1248975619673729\n","0epochs, 200iters - 0.06340835243463516\n","0epochs, 400iters - 0.050388824194669724\n","0epochs, 600iters - 0.0436820387840271\n","0epochs, 800iters - 0.04520810768008232\n","0epochs, 1000iters - 0.04504622891545296\n","0epochs, 1200iters - 0.04993259534239769\n","0epochs, 1400iters - 0.05164537578821182\n","0epochs, 1600iters - 0.056283459067344666\n","0epochs, 1800iters - 0.054112620651721954\n","1epochs, 0iters - 0.03545727580785751\n","1epochs, 200iters - 0.03417545184493065\n","1epochs, 400iters - 0.04556282237172127\n","1epochs, 600iters - 0.04394436627626419\n","1epochs, 800iters - 0.04281645268201828\n","1epochs, 1000iters - 0.052367810159921646\n","1epochs, 1200iters - 0.04269372671842575\n","1epochs, 1400iters - 0.039389774203300476\n","1epochs, 1600iters - 0.038176264613866806\n","1epochs, 1800iters - 0.03709852695465088\n","2epochs, 0iters - 0.03794722631573677\n","2epochs, 200iters - 0.031668733805418015\n","2epochs, 400iters - 0.028505096212029457\n","2epochs, 600iters - 0.03532344847917557\n","2epochs, 800iters - 0.030822906643152237\n","2epochs, 1000iters - 0.03446227312088013\n","2epochs, 1200iters - 0.03838717192411423\n","2epochs, 1400iters - 0.0322791263461113\n","2epochs, 1600iters - 0.02634282037615776\n","2epochs, 1800iters - 0.030830170959234238\n","3epochs, 0iters - 0.022257285192608833\n","3epochs, 200iters - 0.02339782379567623\n","3epochs, 400iters - 0.026674117892980576\n","3epochs, 600iters - 0.03033093363046646\n","3epochs, 800iters - 0.029539804905653\n","3epochs, 1000iters - 0.03162233158946037\n","3epochs, 1200iters - 0.03452052175998688\n","3epochs, 1400iters - 0.028314534574747086\n","3epochs, 1600iters - 0.03402375802397728\n","3epochs, 1800iters - 0.024098185822367668\n","4epochs, 0iters - 0.01725752279162407\n","4epochs, 200iters - 0.019690770655870438\n","4epochs, 400iters - 0.022020813077688217\n","4epochs, 600iters - 0.026847759261727333\n","4epochs, 800iters - 0.02832941524684429\n","4epochs, 1000iters - 0.024831928312778473\n","4epochs, 1200iters - 0.026718422770500183\n","4epochs, 1400iters - 0.020210446789860725\n","4epochs, 1600iters - 0.02438671886920929\n","4epochs, 1800iters - 0.028414297848939896\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9aDyXI9XCLaj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"outputId":"bebe01df-2f51-406f-b280-2dce3e25ed75","executionInfo":{"status":"ok","timestamp":1564562218872,"user_tz":-540,"elapsed":588,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}}},"source":["sentence = \"이 영화를 본 제가 10점 받아야합니다\"\n","okt = Okt()\n","\n","parsed = okt.pos(sentence)\n","parsed"],"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('이', 'Noun'),\n"," ('영화', 'Noun'),\n"," ('를', 'Josa'),\n"," ('본', 'Verb'),\n"," ('제', 'Noun'),\n"," ('가', 'Josa'),\n"," ('10', 'Number'),\n"," ('점', 'Noun'),\n"," ('받아', 'Verb'),\n"," ('야합니다', 'Adjective')]"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"id":"GgwnEiVjJ_aC","colab_type":"code","colab":{}},"source":["parsed = okt.morphs(sentence)\n","\n","vector = np.zeros([1,100])\n","\n","for i, word in enumerate(parsed):\n","  if word in voca2index:\n","    vector[0, i] = voca2index[word] ##이 단어에 해당하는 index를 가져온다!\n","  \n","  else :\n","    vector[0,i] = voca2index['<UNK>']\n","\n","vector = torch.from_numpy(vector).long().to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-6hhu4cLA_n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ab8915be-5a47-4c4d-8392-68e8924882a0","executionInfo":{"status":"ok","timestamp":1564562305667,"user_tz":-540,"elapsed":622,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}}},"source":["regressor(vector)"],"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.5319]], device='cuda:0', grad_fn=<AddmmBackward>)"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"38b7cfd0-b1b0-4ca9-ce4b-63f5dce860f4","executionInfo":{"status":"ok","timestamp":1564562338820,"user_tz":-540,"elapsed":596,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}},"id":"RFkmk72DLoDC","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sentence = \"대존잼\"\n","okt = Okt()\n","\n","parsed = okt.pos(sentence)\n","parsed"],"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('대', 'Modifier'), ('존잼', 'Noun')]"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2aeGEvPCLoDL","colab":{}},"source":["parsed = okt.morphs(sentence)\n","\n","vector = np.zeros([1,100])\n","\n","for i, word in enumerate(parsed):\n","  if word in voca2index:\n","    vector[0, i] = voca2index[word] ##이 단어에 해당하는 index를 가져온다!\n","  \n","  else :\n","    vector[0,i] = voca2index['<UNK>']\n","\n","vector = torch.from_numpy(vector).long().to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"50712bfb-9855-4b80-b3d4-e99e0b911797","executionInfo":{"status":"ok","timestamp":1564562341023,"user_tz":-540,"elapsed":605,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}},"id":"AEXVkkpsLoDR","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["regressor(vector)"],"execution_count":87,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.8391]], device='cuda:0', grad_fn=<AddmmBackward>)"]},"metadata":{"tags":[]},"execution_count":87}]},{"cell_type":"code","metadata":{"id":"AfJ4vbLHL4Se","colab_type":"code","colab":{}},"source":["class RNNRegressor(nn.Module): ##hidden_dim은 상태벡터\n","    def __init__(self, voca_num, embedding_dim, hidden_dim):\n","        super(RNNRegressor, self).__init__()\n","\n","        self.voca_num = voca_num\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","\n","        self.embedding = nn.Embedding(voca_num, embedding_dim, padding_idx=0)\n","\n","        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True, bidirectional=True, num_layers=2)\n","        \n","        self.linear1 = nn.Linear(2*self.hidden_dim, 10)\n","        self.linear2 = nn.Linear(10, 1)\n","\n","    def forward(self, word_indices):\n","        embs = self.embedding(word_indices)\n","\n","        output, (h_n, c_n) = self.lstm(embs)\n","        output = output[:, -1, :]\n","\n","        output = torch.relu(self.linear1(output))\n","        output = torch.relu(self.linear2(output))\n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNBeaPEWP_B9","colab_type":"code","colab":{}},"source":["import torch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrMjT6NqQRpk","colab_type":"code","colab":{}},"source":["from torch import device"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WonzH3VZQVrG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"64ee7af8-9305-4b98-d33a-3ff23714d280","executionInfo":{"status":"ok","timestamp":1564563564157,"user_tz":-540,"elapsed":701,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}}},"source":[""],"execution_count":114,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"]},"metadata":{"tags":[]},"execution_count":114}]},{"cell_type":"code","metadata":{"id":"NKQehPMTQDEc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"852e204a-412e-4bd7-e22d-103417c9b1ee","executionInfo":{"status":"ok","timestamp":1564563495107,"user_tz":-540,"elapsed":588,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}}},"source":["torch.device"],"execution_count":112,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"]},"metadata":{"tags":[]},"execution_count":112}]},{"cell_type":"code","metadata":{"id":"WpZEsHcDOit1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":239},"outputId":"23725924-2956-4072-d4f3-cfd3bb6d6c45","executionInfo":{"status":"error","timestamp":1564563600924,"user_tz":-540,"elapsed":596,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}}},"source":["device = torch.device('cuda')\n","regressor = RNNRegressor(len(index2voca), 128, 128)\n","regressor = regressor.to(device)\n","\n","mse_loss = nn.MSELoss()\n","optimizer = torch.optim.Adam(regressor.parameters(), lr=0.01)"],"execution_count":115,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-115-8a69660cca54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex2voca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmse_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"]}]},{"cell_type":"code","metadata":{"id":"PouXwkXEOWPq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":633},"outputId":"7f92a48b-119b-434d-9811-414433348c61","executionInfo":{"status":"error","timestamp":1564563087681,"user_tz":-540,"elapsed":40735,"user":{"displayName":"IkJong P","photoUrl":"https://lh5.googleusercontent.com/-XMHI3In5VGY/AAAAAAAAAAI/AAAAAAAATWk/u_tsKGQQW3I/s64/photo.jpg","userId":"13049101358773066030"}}},"source":["epochs = 5\n","\n","regressor.train()\n","for e in range(epochs):\n","    for i, (batch_X, batch_y) in enumerate(train_loader):\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","\n","        pred = regressor(batch_X)\n","\n","        loss = mse_loss(pred, batch_y)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if i % 200 == 0:\n","            loss = loss.item()\n","            print(f'{e} epochs, {i} iters - {loss}')"],"execution_count":94,"outputs":[{"output_type":"stream","text":["0epochs, 0iters - 0.022676661610603333\n","0epochs, 200iters - 0.019887685775756836\n","0epochs, 400iters - 0.016238193958997726\n","0epochs, 600iters - 0.02307809330523014\n","0epochs, 800iters - 0.022165335714817047\n","0epochs, 1000iters - 0.026211656630039215\n","0epochs, 1200iters - 0.018534783273935318\n","0epochs, 1400iters - 0.015489770099520683\n","0epochs, 1600iters - 0.025645263493061066\n","0epochs, 1800iters - 0.03151300549507141\n","1epochs, 0iters - 0.017127765342593193\n","1epochs, 200iters - 0.017290901392698288\n","1epochs, 400iters - 0.017713595181703568\n","1epochs, 600iters - 0.019181987270712852\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-94-b28dc9eda600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2256\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2257\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"pIsFWLI47SHe","colab_type":"code","colab":{}},"source":["class CNNRegressor(nn.Module):\n","    def __init__(self, voca_num, embedding_dim, filter_lengths, filter_num=20):\n","        super(CNNRegressor, self).__init__()\n","        \n","        self.voca_num = voca_num\n","        self.embedding_dim = embedding_dim\n","        self.filter_lengths = filter_lengths\n","        self.filter_num = filter_num\n","        \n","        self.embedding = nn.Embedding(self.voca_num, self.embedding_dim, padding_idx=0)\n","        self.filters = nn.ModuleList([nn.Conv1d(1, self.filter_num, (l, self.embedding_dim)) \\\n","                                      for l in self.filter_lengths])\n","        \n","        self.linear1 = nn.Linear(self.filter_num*len(self.filter_lengths), 10)\n","        self.linear2 = nn.Linear(10, 1)\n","        \n","    def forward(self, words):\n","        embs = self.embedding(words)\n","        embs = embs.unsqueeze(1)\n","        features = [F.relu(conv(embs)) for conv in self.filters]\n","        features = [f.squeeze(3) for f in features]\n","        features = [F.max_pool1d(f, f.size(2)) for f in features]\n","        output = torch.cat(features, dim=1)\n","        output = output.squeeze(2)\n","        output = torch.relu(self.linear1(output))\n","        output = self.linear2(output)\n","        return output\n","    \n","\n","class RNNRegressor(nn.Module):\n","    def __init__(self, voca_num, embedding_dim, hidden_dim, num_layer=2, bidirectional=True):\n","        super(RNNRegressor, self).__init__()\n","        self.voca_num = voca_num\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layer = num_layer\n","        self.bidirectional = bidirectional\n","        \n","        self.embedding = nn.Embedding(voca_num, embedding_dim, padding_idx=0)\n","        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=self.num_layer,\n","                          bidirectional=self.bidirectional, batch_first=True)\n","        self.linear1 = nn.Linear(hidden_dim*(self.bidirectional + 1), 10)\n","        self.linear2 = nn.Linear(10, 1)\n","        \n","    def forward(self, words):\n","        embs = self.embedding(words)\n","        output, h_n = self.gru(embs)\n","        output = output[:, -1, :]\n","        output = torch.relu(self.linear1(output))\n","        output = self.linear2(output)\n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hwk8yysE7hQ3","colab_type":"code","colab":{}},"source":["device = torch.device('cuda')\n","\n","# reg = CNNRegressor(len(index2voca), 128, [2, 3, 4])\n","reg = RNNRegressor(len(index2voca), 256, 256, num_layer=2, bidirectional=True)\n","reg = reg.to(device)\n","\n","criterion = nn.MSELoss()\n","optim = torch.optim.Adam(reg.parameters(), lr=1e-3, weight_decay=1e-6)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J23bWeZM7iOJ","colab_type":"code","colab":{}},"source":["epochs = 10\n","\n","reg.train()\n","for e in range(epochs):\n","    for i, (batch_X, batch_y) in enumerate(train_loader):\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        \n","        predict = reg(batch_X)\n","        \n","        loss = criterion(predict, batch_y)\n","        optim.zero_grad()\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(reg.parameters(), 0.5)\n","        optim.step()\n","        \n","        if i % 100 == 0:\n","            loss = loss.item()\n","            print(f\"{e}epochs, {i}iters - {loss}\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I2TszMau7jZl","colab_type":"code","colab":{}},"source":["total_loss = []\n","test_num = 0\n","l1_loss = nn.L1Loss()\n","\n","with torch.no_grad():\n","    reg.eval()\n","    for i, (batch_X, batch_y) in enumerate(test_loader):\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","\n","        predict = reg(batch_X)\n","        predict = torch.clamp(predict, min=0, max=1)\n","        \n","        loss = l1_loss(predict, batch_y)\n","        loss = loss.item()\n","        batch_size = batch_X.size(0)\n","        test_num += batch_size\n","        total_loss.append(loss*batch_size)\n","        \n","total_loss = np.sum(total_loss)/test_num\n","print(total_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b0mdz_RE7jdl","colab_type":"code","colab":{}},"source":["text = \"이 영화 정말 감동적이다\"\n","parsed = okt.morphs(text)\n","\n","vector = np.zeros((1, train_X.shape[1]))\n","for i, w in enumerate(parsed):\n","    if w in voca2index:\n","        vector[0, i] += voca2index[w]\n","    \n","vector = torch.from_numpy(vector).long().to(device)\n","predict = torch.sigmoid(reg(vector)).item()\n","print(predict)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8klbiE307jhW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfT-htwW7jkM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}