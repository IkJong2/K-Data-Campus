{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20190730_박종익.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YC4BORS_bfe-","colab_type":"text"},"source":["## MLP\n","- XOR 문제가 해결!!\n","\n","오늘 이야기 할 중점적 내용 **목적에 대한 기준**\n","- Cost function\n","  - MSE in Regression\n","  - Cross Entropy in Classification\n","- Objective function (Cost Function!!)\n","  - Gradient Descent\n","  \n","그렇다면 NN에서 학습을 어떻게 하는가??\n","- Backpropagation!!!\n","  - 마치 로피탈의 정리를 알아서 문제는 푸는데, 정확하게는 모른다!!\n","  - 학습을 할 때에는 메모리 소모가 심하다! ... by 중간중간마다 activation\n","  \n","1. Activation functions\n","  - Sigmoid, ReLU\n","  \n","2. Batch normalization\n","\n","3. Optimization methods\n","  - SGD, momentum, Adagrad, RMSProp, Adam, AdaGrad\n","\n","4. Ensemble and regularization\n","  - Dropout, data augmentation, ...\n"," \n","Why Nonlinear??\n","- 왜냐하면 layer를 쌓을 때, linear로 쌓게 되면, 결국 하나로 표현이 가능해져 버린다!\n","\n","### Activation Functions\n","#### Sigmoid\n","$$ \\sigma(x) = 1/(1+e^{-x}) $$\n","이 아이 같은 경우에 0 또는 1로 생각이 가능할 것!\n","1 - on, 0 - off\n","\n","많은 레이어를 쌓을 경우에 GD가 0이 되버려서 \n","Backpropagation이 학습이 되지 않는다... 그 문제가 Vanishing Gradient Problem!!\n","\n","layer를 증가시키니까 학습이 되지 않더라! 그래서 Sigmoid를 버리게 되었다~\n","\n","1. Saturated(0,1로만 구성) neurons 'kill' the gradient\n","2. Sigmoid outputs are not zero-centered\n","  - 양수값 밖에 존재하지 않는다!\n","  - 지그재그 패턴을 가질 수 밖에 없다! \n","  - ... iter 증가 ... 목표치가 아니라 최적(찾고 싶은 Weight)을 찾아가는데 모두 가야함! 그런데 찾는데 오래 걸림\n","3. exp() 은 생각보다 계산량이 많다!! ... 어차피 사용 못하고 테일러급수를 이용해서 다항식으로 표현!\n","\n","#### tanh\n","나온 배경이 결국 zero-centered!!\n","\n","Sigmoid와 tanh의 경우 형제관계라고 보면 된다.\n","\n","서로가 서로로 표현이 가능하다.\n","\n","Sigmoid에 비해 학습이 조금 더 빠르다! because 2*sigmoid\n","\n","$$ tanh(x) = 2 * sigmoid -1 $$\n","\n","- Squashes numbers to range [-1,1]\n","- zero centered(nice)\n","- still kills gradients when saturated :( ㅠㅠ...\n","  - 결국 Vanishing Gradient Problem은 해결 X\n","\n","**Gradient Vanishing??**\n","- weight가 앞 레이어에 전달이 되지 않는 경우!\n","\n","#### ReLU\n","\n","Gradient Vanishing 문제가 해결이 됨!\n","\n","$$ f(x) = max(0,x) $$\n","\n","장점\n","- Does not Saturate!\n","- Very computationally efficient\n","- Converges much faster than sigmoid/tanh in practice(e.g. 6x)\n","\n","단점\n","- Not zero-centered output ...지그재그 path로 가야함\n","- An annoyance\n","\n","Decision Boundary 를 좀 복잡하게\n","\n","how to ReLU??\n","\n","Nonlinear를 ReLU 끊어서 끊어서 조합하게 되면 최대한 비슷하게끔 만들어 줄 수 있다~\n","\n","ReLU 의 경우 Decision Boundary가 다각형 모양이 나오게 된다\n","\n","tanh의 경우 약간 둥글둥글하게 나오긴한다."]},{"cell_type":"markdown","metadata":{"id":"HT83DYNCexQb","colab_type":"text"},"source":["**Actication Function**\n","on - off 인 sigmoid, tanh의 경우 Saturated 되어 버린다!\n","\n","이 부분의 경우 Gradient값이 평이 하게 0이 되어버린다.\n","\n","Objective function (목적함수)\n","\n","기준에 의해서 에러를 역으로 찾아가서 어떤  아이가 많은 영향을 끼치냐?? -> Backpropagation!\n","\n","ReLU의 경우 많이 돌아가긴 하지만 Gradient Vanishing의 문제는 없다! 그래서 오래걸리지만 성능은 더 좋을 수 있다!\n","\n","ReLU의 단점!!\n","- dead ReLU\n","  - ReLU 에서 초기 x들에서 0 값만을 가져온다. 학습을 아무리 하더라도 값을 가지지 않는다!\n","  - 해당 노드가 항상 0의 값을 가지게 되는 것... Gradient가 0이 되어버린다. 업데이트가 되지 않아 학습 X\n","  - 그래서 Bias term을 주게 된다!\n","  $$ w_1x_1 + w_2x_2 + b_1 $$ b1을 bias로 주면서 그나마 살리려고 해보는 것!\n","  \n","  - ReLU Issue?? \n","    1. Weight initialize 를 잘 설정해야만 Dead ReLU를 줄일 수 있다.\n","    2. Layer를 굉장히 많이 늘려버린다! ... 30%가 죽더라도 살았는 것을 가지고 한다.\n","    \n","#### Leaky ReLU, Maxout, ELU\n","- Dead ReLU issue에서 나왔는데, 0인 값들을 죽이지 말자.\n","\n","**왜 저런 Activation Function이 나왔는지에 대해서 큰 흐름을 보면 좋을 것이다**\n"]},{"cell_type":"markdown","metadata":{"id":"bA8iNcb0qC4O","colab_type":"text"},"source":["## Data Preprocessing\n","\n","- Normalization을 왜 하는가??\n","  - 결국은 Scale을 맞춰주기 위함!! Scale을 안 맞춰져있으면, 주어진 데이터를 잘 못 이해할 가능성이 있음\n","  $$w_1x_1 + w_2x_2 $$ 에서 w1 와 w2 space를 맞춰준다고 보면 된다!!\n","  \n","  - 가장 유명한 것은 표준화!\n","  - train data에서 평균, 표준편차를 구한다.\n","  - 각자의 Feature가 Independent하다! 라는 가정을 가지고 한다.\n","\n","- PCA\n","  - 중요한 Feature를 가져오는 것\n","\n","**Tip**\n","1. linear model에서 bias 텀 beta_0는 무조건 있어야 한다!\n","2. linear?? $$f(m+n) = f(m) + f(n) $$\n","  - cf) nonlinear ReLU => f(1) = f(-1) + f(2) => 1 = 0 + 2 이기 때문에 Nonlinear\n","  \n","## Gradient Descent Update\n","- 미분!! ... 개형을 알 수 있다.\n","\n","2가지 챌린지\n","1. local minima를 어떻게 빠져나올 것인지\n","  - Momentum(관성)을 통해서 나갈 수 있다.\n","2. 얼마나 빠르게 갈건지\n","\n","#### Stochastic Gradient Descent\n","- 확률 모델링! Random Process ... 데이터를 임의로 추출하는 과정이 있을 때!\n","- 하나만을 뽑아서 GD를 가는데, 여러번 시도한다! => SGD\n","\n","Stochastic Gradient\n","\n","batch Gradient ... 전체데이터!!\n","- Object Function (Cost Function)이 고정이 되어 버린다\n","- 그리고 local minima에 빠진다..\n","- Computing 속도가 크다...\n","\n","Mini batch Gradient\n","- 다양한 cost function을 가질 수 있다. 그래서 점점 global minimum을 찾아 갈 수 있게 된다.\n","- Local minima를 피할 수 있다.\n","- Computing 속도가 적다.\n","\n","\n","SGD 는 하나를 바탕으로 하는 것\n","\n","Mini batch Gradient (데이터의 일부) SGD와 유사!\n","\n","요즘에 그런데 Batch Gradient는 Mini batch Gradeint이다~\n","\n","거의 평평한 구간에 가게 되면 학습속도가 느리게 진행된다...\n","\n","평평하지 않은 구간에서는 변동성이 엄청 심하다."]},{"cell_type":"markdown","metadata":{"id":"T1s48edeuDQi","colab_type":"text"},"source":["#### Gradient Descent Update\n","x - = learning_rate * dx(Gradient)\n","\n","------------------\n","#### Momentum Update\n","v = mu(momentum) * v - learning_rate * dx(Gradient)\n","\n","x += v\n","\n","#### Adagrad Update\n","cache += ds**2\n","\n","x - = learning_rate * dx / (np.sqrt(cahce) + 1e-7)\n","\n","- gradient가 커지면 커질 수록 cache 또한 커져서 학습이 느려진다.\n","- 뒤의 1e-7의 경우 분모를 0으로 하는 것을 막아줌\n","\n","--------------\n","#### RMSProp Update\n","\n","cache += decay_rate * cache + (1 - decay_rate) * ds**2\n","\n","x - = learning_rate * dx / (np.sqrt(cahce) + 1e-7)\n","\n","- cache를 최근 값들의 일부만을 가지고 사용하겠다.\n","- cache가 계속 증가하는 문제가 사라진다.\n","- Momentum이 여기에서 사용하진 않는다.\n","\n","#### Adam Update\n","- RMSProp + Momentum -> Adam\n","\n","first_moment = 0\n","\n","second_moment = 0\n","\n","while True :\n","  \n","  dx = compute_gradient(x)\n","  \n","  first_moment = beta1(mu) * first_moment(v) + (1 - beta1) * dx ......=> Momentum\n","  \n","  second_moment = beta2 * second_moment + (1 - beta2) * dx * dx ...\n","  \n","  x - = learning_rate + first_moment / np.sqrt(second_moment) + 1e-7 ... => AdaGrad / RMSProp\n"," \n"," -----------\n"," 각각의 차이점 잘 체크해서 공부하기!"]},{"cell_type":"markdown","metadata":{"id":"96nxSLxFFLgP","colab_type":"text"},"source":["미분을 하는 이유?? 테일러 급수!!\n","\n","우리가 하고 있는 Gradient는 단지 1차이다~\n","\n","### Learning Rate\n","- 정말 크면 오히려 loss가 커짐 (노란색)\n","- 너무 작으면 엄청 느리게 간다 (파란색)\n","- 적당히 크면 loss가 줄어들지 않고 계속 양옆으로 왔다갔다 (초록색)\n","- 제일 적당함 (빨간색)\n","\n","코딩을 잘했는데, 성능이 별로 좋지 않다?? 80% 정도 Optimize(Hyper parameter)때문!\n","\n","learning-rate 를 loss 가 어느정도 수렴해갈 때, 높여주고~ 수렴해갈 때, 높여주고 ... \n","\n","=> Learning-rate Scheduler\n","\n","1. ReLU 사용\n","2. Adam Optimizer\n","3. Learning Rate Scheduler"]},{"cell_type":"markdown","metadata":{"id":"M89GyX_YFMYg","colab_type":"text"},"source":["모델의 성능을 평가할 때에\n","\n","training accuracy, training loss를 가지고 판단하면 안된다!\n","\n","전체를 100이라고 했을 때, 70이 train, 20이 validation, 10이 text\n","\n","validation 을 가지고 모델성능을 평가하고, tuning을 해야한다.\n","\n","### Regularization\n","\n","#### Weight Regularization\n","\n","- L1, L2 regularization\n","\n","결국 weight에 어느정도 규제를 주겠다~\n","\n","그럼으로써 너무 train 데이터를 따라가게 하지는 않겠다.\n","\n","#### Dropout \n","... Ensemble과 비슷\n","\n","어떤 Feature를 빼고 계산해봐~\n","\n","어떤 노드를 순간적으로 꺼준다. 그래서 나머지 노드만으로 학습을 진행\n","\n","x1, x2, x4만을 가지고 학습\n","\n","x1, x3, x4만을 가지고 학습 \n","\n","등등\n","\n","Ensemble - 민주주의는 승리한다. ㅋㅋㅋㅋ\n","\n","여러 모델들을 만들어서 합치는 method\n","\n","train 할 때에는 Dropout을 적용할 때, Scaling Up을 시켜야 한다.\n","\n","ex) w1x1 + w2x2 + (w3x3) + w4x4 = y(train) *4/3\n","ex) w1x1 + w2x2 + w3x3 + w3x4 = y(test)\n","\n","train에 dropout을 키고, **test때에는 dropout을 무.조.건 꺼야한다.**"]},{"cell_type":"markdown","metadata":{"id":"OBj5e0lrKnaU","colab_type":"text"},"source":["## Batch Normalization\n","\n","- Improves gradient flow through the network\n","- Allows higher learning rates\n","- regularization\n","  - 데이터가 배치이다. 그 데이터에서 하나만을 빼면 Drop out적은 효과가 존재한다\n","  \n","Batch Normalization의 단점을 한마디로!!  \n","  \n","**train 과 test 의 분포는 다를 때 커버치지 못한다.**\n"," \n"," 다시보고 내가 정리하자... 수업을 많이 놓쳤다 ㅠㅠ\n"," \n"," ## Normalization 에는 종류가 엄청 많다!!\n"," - Batch Norm, Layer Norm, Instance Norm, Group Norm 등등 엄청 많다~"]},{"cell_type":"markdown","metadata":{"id":"PPsembSWOUuv","colab_type":"text"},"source":["## CNN\n","Convoulution?? => Feature Extraction을 효과적으로 할 수 있는 network 방식\n","\n","이미지로부터 Feature를 뽑는다?? 쉽게 와닿지는 않음\n","\n","이미지에 Kernel들을 곱해주면서 하는 것들!\n","\n","CNN은 조금 더 Deep하게 쌓으면서 좋은 Feature를 뽑는 Network 방식\n","\n","CNN은 다양한 분야에서 쓰인다.\n","\n","1. Classification\n","2. Retrieval ... 이미지 쿼리(검색)가 주어졌을 때, 이 이미지와 가까운 아이들을 찾는 것.\n","  - 서로 다른 정보들에서 Feature space에서 거리가 멀다면, 학습이 잘 된 것이다.\n","  - 가까운 아이들은 비슷한 아이들이다. 이런 것들\n","  \n","CNN은 작은 filter를 순환시키며, 고양이 귀다!, 고양이 눈이다! 고양이 코네? 고양이 발이다!!\n","\n","Generalize가 안되는 것을 filter를 통해 찾아가면서 판별하는 것\n","\n","Convolution area가 계속 통과되다 보면 찾아간다.\n","\n","-----------\n","\n","#### X랑 O랑 두가지의 클래스\n","\n","translation, scaling, rotation, weight\n","\n","pattern들에 focus를 맞춰서 보게 되는 것\n","\n","노란색 박스로 가는 것이 Sliding Window\n","\n","Sliding Window란 오른쪽으로 쭈욱 이동하는 것.\n","\n","비교할 대상(각각의 서로다른 filter)과 비교당할 대상을\n","\n","elementwise로 곱한다음에 **더한다** ... 매트릭스가 나옴\n","\n","그래서 각각의 필터로 나온 점수 매트릭스가 존재할 것\n","\n","이 매트릭스를 가지고 Pooling을 한다. 보통(2 X 2) ... 제일 큰 값을 기준으로 매칭을 시킨다.\n","\n","이렇게 되면 width에서 반토막 height에서 반토막 그래서 1/4가 되는 것\n","\n","Pooling 후 Nonlinearity를 시켜준다 by ReLU 통과! ...음수 값에 대해서 0값을 준다\n","\n","Activation 후에 Pooling matrix가 바뀐다.\n","\n","그래서 Convolution 이 기본으로 3단계로 구성\n","1. Convolution\n","2. ReLu\n","3. Pooling\n","4. Fully connected\n","\n","1 2 1 2 3 1 2 3 이렇게 Deep stacking\n","\n","이렇게 한 이후 Fully connected layer 일렬로 나열!!\n","\n","선의 굵기는 weight!!\n","\n","이렇게 다 준다음에 Sigmoid를 통과 시켜서 1(X),0(O)를 맞추게 하는 것.\n","\n","Decision Boundary를 Weight로써 시각화를 시켜준게 106페이지 슬라이드!\n","\n","이렇게 된 이후에\n","\n","1 2 1 2 3 1 2 3 4 4 를 통해서 Actual answer가 나옴\n","\n","이렇게 되면 loss 값이 나온다.\n","\n","이 loss를 가지고 backprog, 편미분을 해감\n","\n","그럼으로써 Cost function(loss)를 GD를 통해서 최적의 weight를 찾아감\n","\n","- GD는 y축이 error, x축이 weight\n","\n","이미지에서 CNN이 많이 사용되고, Sound에도 많이 사용이 된다.\n","\n","Convolution의 경우 컬럼들이 바뀌어도 상관 없을 때 성능이 크게 좋지만은 않다... \n","\n","왜?? Convolution의 경우 orthogonal하지 않고, 상관관계가 클 때에만 좋다~\n","\n","여기서부터는 channel까지 포함되어서 이야기를 할 것\n","\n","filter의 갯수가 결국 output 채널의 개수가 될 것이다~\n","\n","원래 (size + 2 * padding_size - kernel_size)/s(sliding) +1\n","\n","\n","\n","- Classification을 할 때 우리가 원하는 정보가 무엇일까??\n","  - 우리에게 필요한 정보들만 가지고 올 것!"]},{"cell_type":"markdown","metadata":{"id":"SroXPRHrdDK7","colab_type":"text"},"source":["임의의 Image data가 주어졌을 때, channel은 RGB\n","\n","filter에서 sliding window하기엔 channel을 맞춰줘야함!\n","\n","그리고 결국 output의 개수는 filter의 개수가 될 것!\n","\n","연산이 싹 들어가서 결국 하나의 element를 구하는 것\n","\n","n(원래 사이즈)-k(filter)+2p(padding) / S(stride)   +1\n","\n","여기서 말하는 Feature란 Convolution을 통과한 아이들!\n","\n","- Low-level Feature는 color, line 정도!\n","  - 3*3을 통과했다고 생각 했을 때, 전체이미지를 아직까지 볼 순 없다.\n","- High-level Feature ... 리세티브 필드\n","  - 32*32*3을 4*4*1024로 쭉 늘렸다 생각했을 때,\n","  - 3*3을 통과시켜서  여기서 채널이 곱해지고 더해지는 연산! ... element하나! 3*3*3\n","  - 3*3을 두개를 통과하면 리세티브 필드가 5가 된다.\n","  - 잡는 패턴 자체가 훨씬 복잡하다! but 해석적으로는 조금 떨어지긴 한다.\n","  - 생각보다 우리가 원하는대로 학습이 되진 않는다...\n","  \n","- Deconvolution 을 통해서 각각의 filter들이 시각화가 가능하다!\n","\n","- Padding\n","  - zero padding ... segmentation, classification에서 많이 사용\n","  - mirror padding = reflexion padding\n","  \n","  spatial을 유지하는 패딩 수\n","  \n","  e.g.) F=3 => zero pad with 1 , F=5 => zero pad with 2 , F=7 => zero pad with 3\n","  \n","  \n"," Input volume 32x32x3\n"," \n"," 10개 5 x 5 x 3 filter with stride 1, pad 2\n"," \n"," Output volume size:\n"," \n"," (32+2 x 2-5)/1 +1 = 32 spatially, so\n"," \n"," 32 x 32 x 10\n"," \n"," parameter 개수는 5 x 5 x 3 x 10 에다가 bias 10 개까지 해서 760개!!\n"," \n"," - MAX POOLING\n","  - operation을 하게 되면 다시 돌릴 순 없다.\n","  \n"," "]},{"cell_type":"markdown","metadata":{"id":"U6qICPdUzi7B","colab_type":"text"},"source":["### CNN Architecture\n","\n","1. AlexNet\n","2. VGG\n","3. GoogLeNet\n","4. ResNet\n","\n","- AlexNet ... CNN기반 ImageNet 처음!\n","  - use ReLU\n","  - Nrom layers\n","  - heavy data augmentation\n","  - dropout 0.5\n","  - batch size 128\n","  - SGD Momentum 0.9\n","  - Learning rate, L2 weight, ensemble\n","\n","- VGGNET ... Feature를 잘 뽑음!\n","  - FC(Fully Connected) layer를 쓴다 정도... parameter를 매우 많이 사용 ... 무겁다\n","  - 3 x 3 을 많아 쌓아서 가볍지만 deep하게 쌓았다.\n","  - FC를 할 때, spatial을 쭉 펴줘야 한다! 그래서 weight의 개수가 엄청 많아지고, parameter도 많아짐...\n","  - VGG16 을 조금 더 선호(가볍기에)! 이걸 써서 Feature를 뽑는다는게 중요!\n","  \n","- GoogLeNet\n","  - 1 x 1, 3 x 3, 5 x 5 등 다양하게 사용함으로써 feature encoding을 좁게 혹은 넓게 사용하고자!\n","  - **결과적으로 1 x 1 layer를 parameter를 줄이고자 사용했다!!**\n","  - 1 x 1 conv 는 \"bottleneck\" layer에 많이 사용! ... Demension을 확 줄여줌\n","  - spatial dimension은 조절 하면서 Depth만 줄이는 것\n","  - 중간중간마다 Loss 를 줌 why?? 중간중간마다 시그널을 받아올 수 있게끔!\n","\n","\n","- LesNet\n","  - 여기서 중요한 것은 Residual block 을 사용\n","  - 네트워크를 deep 하게 stacking을 많이 했다는 것\n","  - deep하게 쌓아갈수록! error가 더 커진다 ... overfitting\n","  - 이걸 해결하기 위해서, H(x) = F(x) - x로 H(x)를 찾았었는데, H(x) - x 인 F(x)를 찾음으로써 결국 error를 줄이고자 함\n","  - 인간의 성능을 넘은 네트워크\n","  \n","- Dense Net\n","  - 그냥 Broad 하게 input signal을 받아서, 계속 넘겨줌으로써 ResNet 보다 조금 더 확장된 버전이라 보면 된다.\n","  - 기존의 +의 경우 32 32 256 + 32 32 256 하면 32 32 256이 되는데,\n","  - concat을 하게 되면 32 32 256 concat 32 32 256 하면 32 32 512 가 된다. 단지 channel에서 붙는다.\n","  - conv weight가 곱해지고 더해지기 때문에 결국 element 개수는 같아진다.\n","  "]},{"cell_type":"code","metadata":{"id":"d54HFEoQ9R-n","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}