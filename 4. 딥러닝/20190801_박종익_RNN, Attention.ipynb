{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20190801_박종익_RNN, Attention.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LQ-4pFvQUzoq","colab_type":"text"},"source":["# Attention\n","## Word Embedding\n","\n","  1. Word Embedding이란?\n","    - 컴퓨터에게 단어의 의미를 알려주고 싶다!!\n","    - Many to one, Many to many 등등을 할 때! Word-Embedding을 미리 깔고 한다!\n","\n","- Word Representation 방법\n","  - 기존 방법\n","      - one - hot encoding ... 단어만큼 0을 생성 후, 그 단어 자리에 1을 준다.\n","        - 쉽다! but, 유사도를 고려하지 못한다... 거리가 무.조.건 루트2!\n","        \n","  - 벡터화\n","    - Distributed Representation\n","    - 한 단어를 벡터 형태로 표현\n","    - cat과 kitty는 비슷한 단어이므로 비슷한 단어 표현형을 가짐 => 거리가 가까움\n","    - 'hamburger'는 'cat' 'kitty'와 비슷하지 않으므로 다른 표현형 => 거리가 멀다!\n","    \n","  - 그렇다면 벡터를 어떻게 학습할까??\n","    - e.g. 수능 영어! ... 모든 단어를 알지는 못한다. 주변의 단어들을 보고 유추하여 판단!\n","    - 주어져 있는 주변의 context를 가지고(빈도) 어느 한 단어의 의미가 결정된다.\n","    - 주변 단어로 부터 중심단어를 맞추기! ... 반대로 중심단어를 가지고 주변단어를 볼 수도!\n","    - 개와 고양이는 비슷한 위치에 있어서 서로 바꿔도 큰 문제는 없을 것\n","    - be 동사의 경우 매우매우 가깝다!! by corpus 내에서 비슷한 위치\n"," \n","  - Word2Vec의 특성\n","    - 워드 벡터 ... man -> woman, king - Queen 의 벡터는 거의 같다! 그래서 연산도 가능\n","    - 특성 - Analogy Reasoning\n","    - Corpus가 무엇인지에 따라서 어떻게 학습할지가 정해진다! 보통 위키 or 뉴스\n"," \n","  - Word2Vec 알고리즘 (direct prediction)\n","    - Input Layer : 주변 단어(Context word) ... Output Layer : 중심단어(Center word) ... 그리고 인풋과 아웃풋이 바뀌어도 된다!\n","    - V : vocabulary size, N : embedding dimension\n","    - 하나의 중심단어로 주변단어를 예측하는 것은 사실 잘 안된다... 그래서 loss가 잘 줄어들지 않음\n","      - e.g. 5개 단어로 3 : 중심단어 1245 : 주변단어 .... 1245의 logit을 softmax를 하면 1이 나와야 하는데 잘 안됨...\n","    - 알고리즘이 2000 dim 을 200dim으로 줄이고 다시 2000dim으로 펴준다!\n","    $$ X -> WX -> \\hat h ->W'h -> X' $$\n","    이랬을 때, X와 X'이 거의 같아져야 좋다!\n","    - 2013년의 알고리즘\n","    - Context word를 하기 위해서 window size를 정해준다. 그래서 이 사이즈에 있는 word들에서만, 각각의 확률을 높여주는 방식으로 학습한다.\n","    $$ P(w_{t-2}|w_t), P(w_{t-1}|w_t), P(w_{t+1}|w_t), P(w_{t+2}|w_t) $$ \n","   - \n","  $$ L(\\theta) = \\Pi_t \\Pi_j P(w_{t+j}|w_t ; \\theta) $$\n","   - \n","      $$ J(\\theta) = - {{1} \\over {T}} \\sum_t \\sum_j logP(w_{t+j}|w_t;\\theta) $$\n","      \n","   - I go to the school (Window size 2)\n","      - I (go, to), go (I to the), to (I go the scool), the (go to school), school (to the)\n","      \n","      v_w when w is a center word, u_w when w is a context word\n","      \n","$$ P(o|c) = {{exp(u_o^T v_c)} \\over {\\sum_{w in V} exp(u_w^Tv_c)}  $$\n","      \n","    - window 크기를 정하고,\n","      - Skipgram : 중심 단어로 window 내의 주변단어 (context words)를 예측\n","        - Semantic task에 장점을 보이나, 학습이 느림\n","      - CBOW : 주변 단어들로 중심 단어를 예측\n","        - Syntactic task에 장점을 보이고, 학습이 상대적으로 빠름\n","    - context, center 일 때의 관점이 다르다!!  \n","    \n","- Tip!\n","  - Likelihood\n","    - 머신러닝은 기본적으로 likelihood를 높이는 방향으로 학습한다.\n","    - 주어져 있는 input을 가지고 실제 y를 구할 확률을 곱해주는 것\n","    - likelihood는 곱 그래서 log를 씌우면 계산하기 쉽게 덧셈으로 바뀐다. ... 이 아이가 Object Function이 된다!\n","    - MLE를 찾는 것이 목표!! ... 이때의 theta를 찾는 것\n","    - Likelihood 기반의 모델들은 train 데이터를 가지고 학습시키기 때문에 train 분포를 따라가게 된다. 그래서 Overfitting의 문제가 생길 가능성이 매우 높다.    \n","    \n"]},{"cell_type":"markdown","metadata":{"id":"dz41cHqIU4oI","colab_type":"text"},"source":["###  Negative sampling\n","- Additional efficiency in traning\n","  - window size 외의 아이들을 sampling 한 다음에 연관성이 적은 아이들의 weight를 줄인다...\n","  - 일단 계산량이 늘어나지만 sampling을 더할 수 있는 장점이 있다.\n","\n","## Word count로 유사도 (Count based) ...SVD\n","\n","  - corpus내에서도 다른 center word 뿐만 아니라 context word간의 관계로도, 유사도를 볼 수 있다.\n","  - SVD를 이용하여 Dimension Redcution 이 가능\n","  - 단어 matrix를 SVD를 통하여 유사도를 계산한다. \n","\n","- Count based(SVD) vs Direct prediction(Skip=gram/ CBOW)\n","  - 각각의 장단점 pdf 40p\n","  \n","\n","## GloVe embedding\n","- 제일 많이 쓴다~~\n","- Fast training\n","- Scalable to huge corpora\n","- Good performance even with small corpus and small vectors\n","\n","## Hierarchical Softmax\n","- 궁금하면 보세용\n","\n","**Word Embedding을 함으로써, 성능을 조금 더 키울 수 있다**\n","- word similarity\n","- Machine translation\n","- Part-of-speech tagging and named entity recognition\n","- Sentiment analysis\n","- Clustering\n","- Semantic lexicon building\n","\n","하지만, 최근 추세는 word embedding을 조금씩 안쓰는 추세로 간다. \n","\n","subword 기반으로 하게 될 경우 성능향상에 도움이 될 수 있다 e.g. 카카오 사전"]},{"cell_type":"markdown","metadata":{"id":"rYYYnI_hE-G2","colab_type":"text"},"source":["## RNN\n","\n","one to many - 하나가 여러가지로 가는 것\n","\n","many to one - sentiment 점수 계산하는 것\n","\n","many to many - machin translation\n","\n","pdf 보면서 다시 정리!!\n","\n","$$ h_t = f_w(h_{t-1}, x_t) $$\n","h_t : new state f : w를 포함한 함수 h_t-1 : old state\n","x_t  : input vector at some time step\n","\n","one-hot 이 아닌 embedding을 초기 layer로써 사용할 것!\n","\n","weight은 0으로 initiallize하여서 사용\n","\n","hello를 뽑고 싶을 때!\n","1. test에서는 output layer를 통한 target을 바로 input에 넣어주면 된다. 그래서 inference할 때에는 예측한 것을 계속 넣어줘야한다~\n","\n","2. train할 때에는 무조건 정답(ground truth)을 target 으로 주어야한다! 결국! output layer를 통한 target이 다시 input으로 가지 않는다! 그래서 train data가 나쁠때, 굉장히 위험하다.\n","\n","- BPTT\n","  - 전체를 한번에 다 돌려고 하면 메모리 상의 부담이 너무 크다!!\n","  - 그래서 sequence 를 조금씩 잘라서, hidden state를 통해서 전달하여서 메모리 부담을 조금 줄여주겠다.\n","  - 이 것이 BPTT 알고리즘\n","  \n","- why tanh??\n","  - 언어 모델링의 경우 -(부정)와 +(긍정)가 모두 존재한다. 그래서 relu 보다는 tanh를 쓰는 것 같다.\n","  - State별로 잘 분류를 해준다~~\n","\n","- Vanishing/Exploding Gradient Problem\n","  - state를 타고가면 갈수록 gradient 가 W들의 곱에 의해 0이 되어 버림... e.g. 4번째 전 state가 굉장히 중요할 경우! 사라져버릴 수도...\n","    - e.g. The writer who buy books ??\n","    - are 이 아니라 is가 되어야 한다!!\n","  - Exploding의 경우 Error를 중첩으로 먹으면서(eigen-value가 1보다 큰 값) 너무 커져버릴 수 있다.\n","  \n","  pdf 51p\n","\n","  - Clipping - 어느정도 올라 갔을 때, 최대로 가는 정도를 규제를 걸어버리는 것! but 방향은 유지!!\n","    - cache에서 했던 것과 비슷! ... cache에서 못잡는 부분을 먼저 잡아준다\n","    - RMSProp같은 경우에 사전에 예방은 하지 못한다. but! Clipping의 경우에 사전에 막아준다\n","  \n","  - Vanishing을 해결하고자 LSTM!\n","    - 수업을 듣고 모든 것을 기억하지 말고, 일종의 필기노트를 유지하자!!\n","  \n","## LSTM, GRU\n","- 기존 Vanilla RNN\n","  - 전체를 가지고 다시 학습을 해야한다\n","  \n","- LSTM(조금 더 복잡한 걸 잘할 수 있다), GRU(메모리를 조금 적게 먹고 빠르게!)\n","  - 기존부분은 어느정도 유지하고, 과거의 어느정도만 학습을 다시 학습을 하겠다.\n","  - RESNet(skip connection)에서 다시 올려주는 거랑 비슷하다고 보면 된다~\n","  - Residual connection!\n","  - 얘네 또한 Exploding이 발생을 할 수 있다! 하지만, Clipping을 모두 줘야함\n","  - 학습이 잘 안될때에는 learning rate와 optimize를 잘 조절해 볼것\n","  \n","- 연습할 때에는\n","  - GRU먼저하고, 그 다음 안될 때에 LSTM을 하라~\n","  - 또는 다른 Archietecture를 가지고 오던지!\n","  \n","  \n","- bidirectional - 논란의 여지가 없이 그냥 무.조.건 해야한다.\n","  - the movie was terribly exciting !\n","    - 한방향으로 간다면, movie가 terribly exciting을 수식받지 못한다\n","    - 그리고 terribly가 부정으로 먹어버리고, exciting에서 state가 바뀔 수 있다.\n","    - 그래서 역순으로 RNN을 태워 보내면 어느정도 상호보완적으로 사용할 수 있다.\n","  - bidirectional을 태우고 난 후 hidden state로 써 사용을 할 것!\n","  \n","  "]},{"cell_type":"markdown","metadata":{"id":"thM95MNUZcPS","colab_type":"text"},"source":["## Attention \n","\n","### Neural Machine Translation\n","- Attention이란 매커니즘으로 엄청 Powerful해짐!\n","- 최근에 나온 아키텍처들을 이해하기 훨씬 쉬워진다~\n","\n","------------------\n","\n","- 기본적으로는 Many to Many 이다.\n","  - encoder -> 주어진 문장으로!\n","  - decorder -> 만들어진 맵핑된 아이들을 바탕으로 생성된 것들을 바꾸는 것 e.g. 번역!\n","  \n","- seq2seq model\n","  - 주어져있는 input sequence로 부터 output으로 sequence를 생성해낼 것이다.\n","  - Token\n","    - start\n","    - end\n","    - padding\n","\n","- NMT (Neural Machine Translation)\n","  - Conditional Language Model\n","  $$ P(y|x) $$\n","  - x는 한국어! y는 다음에 어떤 글자가 올지! y_n, y_n+1을 생산해내면서, y를 만들어 나가는 것\n","  - y_1, x로 y_2 ... y_1, y_2,x로 y_3를 만드는 과정들\n","\n","- Attention Mechanism\n","  - LSTM처럼 모두 학습해서 한가지로 가져오지 말고,\n","  - 필요할 때 가져와서(마치 DB) 사용을 하겠다! => Attention\n","  - **Attention : direct하게 connection을 encoder에서 focusing을 하겠다!**\n","  - 하나의 벡터마다 중요도를 계산해서 Context 벡터를 계산하겠다. 이게 encoder 정보!\n","  - DB적으로 생각! 우리가 필요한 query가 있을 것이다. 거기에 해당하는 key가 얼마나 중요한지(유사도) 비교하여 weight를 주어서 가져오겠다.\n","  - 결국 이런식으로 모델링을 한것~~ 필요할 때마다 direct하게 정보를 빼오게 된다.\n","  - 조금더 길기 때문에 생겼던 Gradient Vanishing을 집중을 시켜서 줄일 수 있다.\n","  ------------------\n","  \n","  - 그래서 이제 유사도를 어떻게 구할 것인가??\n","  - Loung Model\n","    1. 주어진 given sentence를 통해 RNN을 돌려서 encoding vector화를 시킴!\n","    2. Hidden state로써 sequence를 만듬! 이를 통해 decorder의 hidden state\n","    3. 비교를 하면서 만들어지는 알고리즘이 유사도 알고리즘! (dot product)\n","    4. Attention 값을 통해 logit -> softmax -> Context vector\n","    5. Decoder hiddenstate Concat을 한 다음 Linear를 통해서 Vocab을 만든다.\n","    \n","    - 유사도를 구한 다음에 Attention값들을 통해 \n","    - logit 값을 구한다.\n","    - softmax를 해야함!\n","    - softmax 한 결과로 Context vector를 만든다\n","    - Concat을 \n","    - Linear -> Vocab\n"," \n","- Decoder의 hidden state\n","  - Encoder에서 만들어진 hidden-state!\n","  - 이걸 가지고 와서 input값을 통해서 만든게 decorder hidden state!!\n","  - 이 아이가 Query로써 Encorder vector를 참조하게 된다.\n"," \n"," - Attention is great!!\n","  - improves NMT performance\n","  - solves the bottlenect problem\n","  - helps with vanishing gradeint problem\n","  - **Some interpretability**\n","    - 마치 alignment를 조절할 수 있다.\n","\n","- Attention variants\n","  - Basic dot-product\n","  - Multiplicative ... bilinear ... weight를 줘서 하는 것\n","  - Additive ... concat을 통해서 하는 것\n","\n","- Concat?\n","  - 딥러닝에선 덧셈!\n","  - 일단 늘려준 다음에 transpose를 통해서 계산\n","\n","- Vision 쪽에서 Captioning은??\n","  - 필요한 정보들을 이미지에서 CNN 에서 추출!\n","  - Depth에서 어떤 부분에 Attention을 할지? e1, e2, e_wh 들은 depth를 포괄하는 아이들\n","  - Features : L(width) x D(height) -> h0 (query) 를 통해서 a1 유사도 매트릭스가 나옴\n","  - 이걸로 z1을 만들어서 또 만들고 넘기고 만들고 넘기고 등등등\n","  "]},{"cell_type":"markdown","metadata":{"id":"fnatU6ZHvgpc","colab_type":"text"},"source":["## 딥러닝 전체적인 큰 흐름\n","\n","Linear-layer -> Backpropagation이 어떻게 흐르는지 보기 위해~\n","\n","output이 label과 얼마나 틀렸는지!(loss) 노드가 얼만큼 공헌했는지 => backpropagation!\n","\n","이게 되면서 NN이 학습이 가능해진 것!\n","\n","Linear layer 기반의 모델들은 이미지에서 한계가 존재\n","\n","MNIST에서 일렬로 쭉 피게 되었다~ 이건 공간정보를 잃게 된다~\n","\n","Convoluation의 경우 Dependency까지 해주게 된다. ... Spatial한 정보까지 보여주게 된다.\n","\n","... Conv는 중심을 기반으로 돌아가게 된다.\n","\n","이걸 바탕으로 특정한 feature인 대각선, 직선, 세모, 별표 모양을 인식하게 된다.\n","\n","각각의 Conv의 Depth가 Feature가 그 공간에 존재하냐 존재하지 않냐를 알게 된다.\n","\n","이 Depth에서 High level Feature를 알 수 있게 된다.\n","\n","Depth와 주변정보를 가지고 output을 낼 수 있게 되었다.\n","\n","딥러닝 모델이 왜 좋으냐??\n","\n","-> Representation Learning\n","\n","데이터만 주면 알아서 눈, 코, 입 같은 Feature를 알게 된다!\n","\n","CNN Layer를 쭉 쌓으면 해당되는 Feature extraction을 해준다\n","\n","이걸 FC layer를 거쳐서 Logit 값을 얻게 되었다. 이를 통해 classification을 할 수 있게 되었다.\n","\n","- Non-linear <=> linear\n","\n","  - Non-linear를 통해 복잡한 것 할 수 있게 되었다.\n","\n","- Optimize\n","  - SGD\n","    - optima의 문제\n","    - Momentum을 통해 해결\n","    - over shooting -> RMSProp!\n","    - 이걸 합친 Adam!!\n","\n","- dropout / ensenble/ batch_Normalization\n","\n","- RNN\n","\n","- Word Embedding\n","  - one-hot보다는 벡터기반이 훨씬 학습에 좋다!\n","  - 주변 단어로 부터 그 단어를 유추할 수 있다. 비슷비슷하고, 빈도가 비슷비슷하다로 부터 학습하게끔!\n","  - 그리고 빈도만을 통해서 할 수도 있다!\n","\n","## Weight Initialization\n","- 이걸 잘해야 모델이 잘 학습한다~\n","1. Small random numbers (평균 0, var)\n","  - layer를 거치면 거칠수록, activation 값이 거의 0이 되어버린다. (var 이 작을때)\n","  - var을 키우니까 tanh를 거치면-1 과 1 값을 대부분 가지게 됨... => Gradient Vanishing\n","\n","2. Xiavier initialization - 가장 좋다! CNN FFNN (Feed Forward Neural Network)\n","  - 랜덤을 줄 때, sqrt(input_dim) 으로 나눠줌! 마치 표본평균의 표준편차\n","  - 훠어어어얼씬 안정화가 되고, 정규분포화가 된다~\n","  - tanh나 sigmoid는 해결이 되지만 ReLU는 해결이 안됨! (by 음수는 싹 다 0)\n","  - ReLU해결? \n","    - 랜덤을 줄 때, sqrt(input_dim) 으로 나눠주고 x2 를 해준다!!\n","    - 이렇게 하게되면 ReLU가 훨씬 학습이 안정화가 되게 된다\n","  \n","- RNN 이나 GRU의 경우\n","  - orthogonal initialize가 대체로 좋다~\n","  - 최대한 많은 정보가 다음으로 전달되게끔 해준다!\n"]},{"cell_type":"code","metadata":{"id":"W25EFTZSYqIm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}